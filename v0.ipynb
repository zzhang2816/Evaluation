{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import os\n",
    "torch.manual_seed(42)\n",
    "from utils import generate_dataloaders\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzzhang2816\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=np.load(\"dataset/train.npz\")\n",
    "val_set=np.load(\"dataset/val.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: the shape of X: torch.Size([32, 8, 49]); the shape of y: torch.Size([32, 1])\n",
      "VAL: the shape of X: torch.Size([32, 8, 49]); the shape of y: torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader=generate_dataloaders(train_set, val_set,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"The RNN model.\"\"\"\n",
    "    def __init__(self, input_dimension,output_dimension,num_layers,num_hiddens):\n",
    "        super().__init__()\n",
    "        self.input_dimension = input_dimension\n",
    "        self.output_dimension=output_dimension\n",
    "        self.num_hiddens=num_hiddens\n",
    "        self.rnn = nn.LSTM(self.input_dimension, self.num_hiddens,num_layers)\n",
    "        self.linear = nn.Linear(self.num_hiddens, self.output_dimension)\n",
    "\n",
    "    def forward(self, inputs,state=None):\n",
    "        X = inputs.permute(1,0,2)\n",
    "        X = X.to(torch.float32)\n",
    "        _, state = self.rnn(X)\n",
    "        # if state is not None:\n",
    "        #     _,state = self.rnn(X, state)\n",
    "        # else:\n",
    "        #     _, state = self.rnn(X)\n",
    "        output = self.linear(state[-1][-1]) # take the hidden state 2, on the layer 2\n",
    "        return output\n",
    "    \n",
    "    # def begin_state(self, device, batch_size=1):\n",
    "    #     if not isinstance(self.rnn, nn.LSTM):\n",
    "    #         # `nn.GRU` takes a tensor as hidden state\n",
    "    #         return torch.zeros((self.rnn.num_layers,\n",
    "    #                             batch_size, self.num_hiddens), device=device)\n",
    "    #     else:\n",
    "    #         # `nn.LSTM` takes a tuple of hidden states\n",
    "    #         return (torch.zeros((self.rnn.num_layers,\n",
    "    #                              batch_size, self.num_hiddens),\n",
    "    #                             device=device),\n",
    "    #                 torch.zeros((self.rnn.num_layers,\n",
    "    #                              batch_size, self.num_hiddens),\n",
    "    #                             device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, num_epochs, device,train_loader, val_loader, load_from_path,save_to_path):\n",
    "        if load_from_path:\n",
    "                net.load_state_dict(torch.load(load_from_path))\n",
    "        if not os.path.isdir(save_to_path):\n",
    "            os.mkdir(save_to_path)\n",
    "        loss = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "        \n",
    "        # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optim, 'min',factor=0.5, verbose = True, min_lr=1e-6, patience = 5)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 25)\n",
    "\n",
    "        # saved_name=f'fold {fold_i}.pt'\n",
    "        # early_stopping = EarlyStopping(model_save,saved_name,patience = 10, verbose=True)\n",
    "\n",
    "        net.to(device)\n",
    "        \n",
    "        epoch_trainlosses=[]\n",
    "        epoch_vallosses=[]\n",
    "        for epoch in range(num_epochs):\n",
    "            for (dataset, loader) in [(\"train\", train_loader), (\"val\", val_loader)]: \n",
    "                if dataset == \"train\":\n",
    "                        torch.set_grad_enabled(True)\n",
    "                        net.train()\n",
    "                else:\n",
    "                        torch.set_grad_enabled(False)\n",
    "                        net.eval()\n",
    "                total_epoch_loss = 0\n",
    "                for batch_idx, (X,y) in enumerate(loader): \n",
    "                    X=X.to(device)\n",
    "                    y=y.to(device)\n",
    "\n",
    "                    y_hat=net(X)\n",
    "                    l=loss(y_hat,y)\n",
    "                    \n",
    "                    total_epoch_loss += l.cpu().detach().numpy()*X.shape[0]\n",
    "                    if(batch_idx%100==0):\n",
    "                        message=\"\"\n",
    "                        message += f\"Epoch {epoch+1}/{num_epochs} progress: {int((batch_idx / len(loader)) * 100)}% \"\n",
    "                        message += f'loss: {l.data.item():.4f}'\n",
    "                        print(message)\n",
    "                        # wandb.log({\"message\": message})\n",
    "\n",
    "                    if dataset == \"train\" :\n",
    "                        optimizer.zero_grad()\n",
    "                        l.backward()\n",
    "                        optimizer.step()\n",
    "          \n",
    "                avg_epoch_loss = total_epoch_loss/ len(loader.dataset)\n",
    "                if dataset == \"train\" :\n",
    "                    epoch_trainlosses.append(avg_epoch_loss)\n",
    "                if dataset == 'val':\n",
    "                    epoch_vallosses.append(avg_epoch_loss)\n",
    "                \n",
    "                # print(f'Epoch: {epoch}; Avg_loss: {avg_epoch_loss}')\n",
    "            wandb.log({\"train_loss\": epoch_trainlosses[-1],\"val\":epoch_vallosses[-1]})\n",
    "\n",
    "            # epoch_valloss=epoch_vallosses[-1]\n",
    "            scheduler.step()        \n",
    "            # early_stopping(epoch_valloss, model)\n",
    "            # if early_stopping.early_stop:\n",
    "            #     print(\"Early stopping\")\n",
    "            #     break\n",
    "            if epoch%10 == 0:\n",
    "                torch.save(net.state_dict(), save_to_path+f\"{epoch}.pt\")\n",
    "        if epoch%10!=0:\n",
    "            torch.save(net.state_dict(), save_to_path+f\"{num_epochs}.pt\")\n",
    "        # wandb.finish()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:223udh1g) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3689... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141d4fd960f24ced8fa7cb6d0f416445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▃▃▂▂▁▁▁▁▁</td></tr><tr><td>val</td><td>█▅▃▃▂▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
       "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>message</td><td>Epoch 10/10 progress...</td></tr><tr><td>train_loss</td><td>22.73558</td></tr><tr><td>val</td><td>26.62013</td></tr></table>\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">wobbly-sun-4</strong>: <a href=\"https://wandb.ai/zzhang2816/trial/runs/223udh1g\" target=\"_blank\">https://wandb.ai/zzhang2816/trial/runs/223udh1g</a><br/>\n",
       "Find logs at: <code>./wandb/run-20211225_101225-223udh1g/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:223udh1g). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/zzhang2816/trial/runs/248vik6p\" target=\"_blank\">decent-field-5</a></strong> to <a href=\"https://wandb.ai/zzhang2816/trial\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/zzhang2816/trial/runs/248vik6p?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fb138bcfb50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"trial\", config={'lr':0.001,'num_layers':2,'num_hiddens':8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNNModel(input_dimension=49,output_dimension=1,num_layers=2,num_hiddens=8)\n",
    "num_epochs=5\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "load_from_path=None\n",
    "save_to_path=\"checkpoints/v0/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(net, num_epochs, device,train_loader, val_loader, load_from_path,save_to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eb3ee2cb987f0791d59217aac9c93edb0025d3eec1e91adab0e3ff51219fcd98"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('areix': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
